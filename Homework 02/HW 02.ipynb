{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 2\n",
    "## problem 1\n",
    "Show that the stationary point (zero gradient) of the function <br>\n",
    "$ f(x_1,x_2) = 2x_1^2 -4x_1x_2 + 1.5x_2^2 + x_2 \\ $\n",
    "is a saddle (with indefinite Hessian).\n",
    "\n",
    "$ g(x_1,x_2) = \\begin{bmatrix} 4x_1 - 4x_2 \\\\ -4x_1 + 3x_2 + 1 \\end{bmatrix} $ <br>\n",
    "\n",
    "$ H(x_1,x_2) = \\begin{bmatrix} 4 & -4 \\\\ -4 & 3 \\end{bmatrix} $ <br>\n",
    "$ | H - \\lambda I | = 0 $ <br>\n",
    "$ \\begin{vmatrix} 4-\\lambda & -4 \\\\ -4 & 3-\\lambda \\end{vmatrix} = 0 = (4-\\lambda)(3-\\lambda) - (-4*-4)$\n",
    "$ = 12 - 7\\lambda + \\lambda^2 - 16 \\\\ \\quad \\lambda^2 - 7\\lambda - 4 = 0$\n",
    "$ \\lambda = 7.5311\\  ;\\ -0.5311$\n",
    "The Hessian of this function has both negative and positive eigen values. This means the Hessian is indefinite and the function has a saddle point. This saddle point occurs at (1,1). <br><br>\n",
    "\n",
    "Taylor Series expansion <br>\n",
    "$f(x_1,x_2) = f(1,1) + g|^T_{(1,1)}\\begin{bmatrix} x_1-1 \\\\ x_2-1\\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} x_1-1 \\\\ x_2-1\\end{bmatrix} ^T \\begin{bmatrix} 4 & -4 \\\\ -4 & 3\\end{bmatrix} \\begin{bmatrix} x_1-1 \\\\ x_2-1\\end{bmatrix} $ <br>\n",
    "g(X) evaluated at (1,1) is equal to zero so it drops out. Doing the vector multiplication:<br>\n",
    "$ f(x_1,x_2) - f(1,1) = \\frac{1}{2}  \\left( 4( \\partial x_1)^2 - 4 \\partial x_1 \\partial x_2 - 4\\partial x_1 \\partial x_2 + 3(\\partial x_2)^2 \\right) $\n",
    "Where $ \\partial x_i = x_i -1 \\ $ for $i=1,2 $ <br>\n",
    "The right hand side of the equation can be factored into:\n",
    "$ \\frac{1}{2} (2\\partial x_1 - 3\\partial x_2)(2\\partial x_1 - \\partial x_2) $\n",
    "The downward slopes occur when $ \\frac{1}{2} (2\\partial x_1 - 2\\partial x_2)(2\\partial x_1 - \\partial x_2) < 0$\n",
    "There is a downward slope when only one factor is negative.\n",
    "$ 2\\partial x_1 <\\partial x_2 \\quad 2\\partial x_1 < \\partial x_2 $ <br>\n",
    "so Any direction between vectors <3,2> and <1,2> or the negative counterparts has a downward slope."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Probelm 2\n",
    "Find the point in the plane $ x_1 + 2x_2+3x_2 = 1 \\ $ in $\\mathbb{R}^3\\ $ that is nearest to the point $(-1,0,1)^T\\ $ Is this a convex problem? <br><br>\n",
    "\n",
    "The distance formula can be squared to create a sum of squares. point $(-1,0,1)^T\\ $ is used in this formula as a point:\n",
    "\\begin{equation} distance^2 = (x_1 +1)^2 + (x_2)^2 + (x_3-1)^2\\end{equation} <br>\n",
    "This equation requires constraints in order to work. To make it an unconstrained problem, one x can be found in terms of the other x's. The possible equations are as follows:<br>\n",
    "$ x_1 = 1 - 2x_2 - 3x_3 \\\\ x_2 = \\frac{1}{2} - \\frac{1}{2}x_1 - \\frac{3}{2}x_3 \\\\ x_3 = \\frac{1}{3} - \\frac{1}{3}x_1 - \\frac{2}{3}x_2 $\n",
    "<br>\n",
    "In an effort to avoid fractions, $x_1= f(x_2,x_3)\\ $\n",
    "\\begin{equation} (2 - 2x_2-3x_3)^2 + (x_2)^2 + (x_3-1)^2 \\end{equation}\n",
    "This function is a convex function. Because each squared term is a polynomial with a positive squared highest order term (when all multiplied out), each piece is concave up and the Hessian will be positive definite.This also has the effect of making the equation a 2 dimensional minimization. The gradient and Hessian are as follows.\n",
    "\\begin{equation} g(x) = \\begin{bmatrix} -4(2-2x_2-3x_3) + 2x_2 \\\\ -6(2-2x_2-3x_3)+2(x_3-1) \\end{bmatrix} = \\begin{bmatrix} -8+10x_2+12x_3 \\\\ -14 + 12x_2+20x_3 \\end{bmatrix} \\end{equation}\n",
    "\\begin{equation} H(x) = \\begin{bmatrix} 10 & 12 \\\\ 12 & 20 \\end{bmatrix} \\end{equation}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 48>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwas a success: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(success) \u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mx values = \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(x0)\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMinimum distance: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(distance))\u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x0\n\u001B[1;32m---> 48\u001B[0m xtrial \u001B[38;5;241m=\u001B[39m \u001B[43mgradient_decent_inexact_line_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdistance_sq\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdistance_sq_g\u001B[49m\u001B[43m,\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03mx0 = [1, 2]\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;124;03mresgrad = opt.minimize(distance_sq,x0,method='BFGS' , jac= distance_sq_g,options={'display':True, 'return_all':True})\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;124;03mxfinal =resnewton.x\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;124;03mprint(xfinal) '''\u001B[39;00m\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mgradient_decent_inexact_line_search\u001B[1;34m(function, gradient, x0, t, tollerance)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m g0norm \u001B[38;5;241m>\u001B[39m tollerance \u001B[38;5;129;01mand\u001B[39;00m counter\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m100\u001B[39m:\n\u001B[0;32m     40\u001B[0m     x0[counter] \u001B[38;5;241m=\u001B[39minexact_line_search(function,g0,x0,t)\n\u001B[1;32m---> 41\u001B[0m     g0 \u001B[38;5;241m=\u001B[39m gradient(\u001B[43mx0\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     42\u001B[0m     g0norm \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(g0)\n\u001B[0;32m     43\u001B[0m     counter \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m counter\n",
      "\u001B[1;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as ppl\n",
    "from scipy import optimize as opt\n",
    "\n",
    "\n",
    "# defines the distance squared function\n",
    "def distance_sq(x):\n",
    "    eq = (2 - 2 * x[0] - 3 * x[1]) ** 2 + x[0] ** 2 + (x[1] - 1) ** 2\n",
    "    return eq\n",
    "\n",
    "\n",
    "# defines the gradient of the distance squared function\n",
    "def distance_sq_g(x):\n",
    "    der = np.zeros_like(x)\n",
    "    der[0] = -8 + 10 * x[0] + 12 * x[1]\n",
    "    der[1] = -14 + 12 * x[0] + 20 * x[1]\n",
    "    return der\n",
    "\n",
    "\n",
    "def distance_sq_H():\n",
    "    H = np.zeros((2, 2))\n",
    "    H = H + np.diag([10, 20])\n",
    "    H[0, 1] = 12\n",
    "    H[1, 0] = H[0, 1]\n",
    "    return H\n",
    "\n",
    "\n",
    "def inexact_line_search(function, g0, x0, t):\n",
    "    alpha = 1\n",
    "    counter = 0\n",
    "    func_eval = function(x0 - alpha * g0)\n",
    "    phi_eval = function(x0) - t * g0.T @ g0 * alpha\n",
    "    while func_eval > phi_eval and counter < 100:\n",
    "        alpha = alpha / 2\n",
    "        counter += 1\n",
    "        func_eval = function(x0 - alpha * g0)\n",
    "        phi_eval = function(x0) - t * g0.T @ g0 * alpha\n",
    "    xnew = x0 - alpha * g0\n",
    "    return xnew\n",
    "\n",
    "\n",
    "def gradient_decent_inexact_line_search(function, gradient, x0, t, tollerance):\n",
    "    counter = 0\n",
    "    g0 = gradient(x0)\n",
    "    g0norm = np.linalg.norm(g0)\n",
    "    while g0norm > tollerance and counter < 100:\n",
    "        counter += 1\n",
    "        x0 = inexact_line_search(function, g0, x0, t)\n",
    "        g0 = gradient(x0)\n",
    "        g0norm = np.linalg.norm(g0)\n",
    "    success = g0norm < tollerance\n",
    "    distance = function(x0)\n",
    "    print(\"was a success: \" + str(success) + \"\\nx values = \"), print(x0), print(\"\\nMinimum distance: \"), print(distance)\n",
    "    return x0\n",
    "\n",
    "\n",
    "x0 = np.array([1, 1])\n",
    "xtrial = gradient_decent_inexact_line_search(distance_sq, distance_sq_g, x0, 0.5, 1e-3)\n",
    "\n",
    "'''\n",
    "x0 = [1, 2]\n",
    "resgrad = opt.minimize(distance_sq,x0,method='BFGS' , jac= distance_sq_g,options={'display':True, 'return_all':True})\n",
    "xtrial = resgrad.allvecs\n",
    "xfinal = resgrad.x\n",
    "resgrad\n",
    "print(xfinal)\n",
    "resnewton = opt.minimize(distance_sq,x0, method='Newton-CG', jac=distance_sq_g,hess=distance_sq_H, options={'xtol':1e-8, 'disp':True})\n",
    "xfinal =resnewton.x\n",
    "print(xfinal) '''\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 3\n",
    "Prove that a hyperplane is a convex set\n",
    "A hyperplane can be defined using $ a^Tx = c \\ $ for $x \\in \\mathbb{R}^n\\ $ where a is the normal direction of the hyperplane and c is some constant.<br>\n",
    "$a^Tx = c \\ $ is a linear equation as it would multiply out as follows:\n",
    "\\begin{equation} a_1x_1 + a_2x_2 + ... +a_nx_n = c \\end{equation} this is a linear equation and linear equations contain convex sets.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
