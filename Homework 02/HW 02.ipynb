{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 2\n",
    "## problem 1\n",
    "Show that the stationary point (zero gradient) of the function <br>\n",
    "$ f(x_1,x_2) = 2x_1^2 -4x_1x_2 + 1.5x_2^2 + x_2 \\ $\n",
    "is a saddle (with indefinite Hessian).\n",
    "\n",
    "$ g(x_1,x_2) = \\begin{bmatrix} 4x_1 - 4x_2 \\\\ -4x_1 + 3x_2 + 1 \\end{bmatrix} $ <br>\n",
    "\n",
    "$ H(x_1,x_2) = \\begin{bmatrix} 4 & -4 \\\\ -4 & 3 \\end{bmatrix} $ <br>\n",
    "$ | H - \\lambda I | = 0 $ <br>\n",
    "$ \\begin{vmatrix} 4-\\lambda & -4 \\\\ -4 & 3-\\lambda \\end{vmatrix} = 0 = (4-\\lambda)(3-\\lambda) - (-4*-4)$\n",
    "$ = 12 - 7\\lambda + \\lambda^2 - 16 \\\\ \\quad \\lambda^2 - 7\\lambda - 4 = 0$\n",
    "$ \\lambda = 7.5311\\  ;\\ -0.5311$\n",
    "The Hessian of this function has both negative and positive eigen values. This means the Hessian is indefinite and the function has a saddle point. This saddle point occurs at (1,1). <br><br>\n",
    "\n",
    "Taylor Series expansion <br>\n",
    "$f(x_1,x_2) = f(1,1) + g|^T_{(1,1)}\\begin{bmatrix} x_1-1 \\\\ x_2-1\\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} x_1-1 \\\\ x_2-1\\end{bmatrix} ^T \\begin{bmatrix} 4 & -4 \\\\ -4 & 3\\end{bmatrix} \\begin{bmatrix} x_1-1 \\\\ x_2-1\\end{bmatrix} $ <br>\n",
    "g(X) evaluated at (1,1) is equal to zero so it drops out. Doing the vector multiplication:<br>\n",
    "$ f(x_1,x_2) - f(1,1) = \\frac{1}{2}  \\left( 4( \\partial x_1)^2 - 4 \\partial x_1 \\partial x_2 - 4\\partial x_1 \\partial x_2 + 3(\\partial x_2)^2 \\right) $\n",
    "Where $ \\partial x_i = x_i -1 \\ $ for $i=1,2 $ <br>\n",
    "The right hand side of the equation can be factored into:\n",
    "$ \\frac{1}{2} (2\\partial x_1 - 3\\partial x_2)(2\\partial x_1 - \\partial x_2) $\n",
    "The downward slopes occur when $ \\frac{1}{2} (2\\partial x_1 - 2\\partial x_2)(2\\partial x_1 - \\partial x_2) < 0$\n",
    "There is a downward slope when only one factor is negative.\n",
    "$ 2\\partial x_1 <\\partial x_2 \\quad 2\\partial x_1 < \\partial x_2 $ <br>\n",
    "so Any direction between vectors <3,2> and <1,2> or the negative counterparts has a downward slope."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Probelm 2\n",
    "Find the point in the plane $ x_1 + 2x_2+3x_2 = 1 \\ $ in $\\mathbb{R}^3\\ $ that is nearest to the point $(-1,0,1)^T\\ $ Is this a convex problem? <br><br>\n",
    "\n",
    "The distance formula can be squared to create a sum of squares. point $(-1,0,1)^T\\ $ is used in this formula as a point:\n",
    "\\begin{equation} distance^2 = (x_1 +1)^2 + (x_2)^2 + (x_3-1)^2\\end{equation} <br>\n",
    "This equation requires constraints in order to work. To make it an unconstrained problem, each x can be found in terms of the other x's. The equations are as follows:<br>\n",
    "$ x_1 = 1 - 2x_2 - 3x_3 \\\\ x_2 = \\frac{1}{2} - \\frac{1}{2}x_1 - \\frac{3}{2}x_3 \\\\ x_3 = \\frac{1}{3} - \\frac{1}{3}x_1 - \\frac{2}{3}x_2 $\n",
    "<br>\n",
    "These can then be plugged into the first equation to make the equation an unconstrained equation.\n",
    "\\begin{equation} (2 - 2x_2-3x_3)^2 + \\frac{1}{4}(1 - x_1 - 3x_3)^2 + \\frac{1}{9}(-2 -x_1 -2x_2)^2 \\end{equation}\n",
    "This function is a convex function. Because each squared term is a polynomial with a positive squared highest order term (when all multiplied out), each piece is concave up and the Hessian will be positive definite if I calculated it out."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trevo\\AppData\\Local\\Temp\\ipykernel_12716\\2371105701.py:28: OptimizeWarning: Unknown solver options: display\n",
      "  resgrad = opt.minimize(distance_sq,x0,method='BFGS' , jac= distance_sq_g,options={'display':True, 'return_all':True})\n"
     ]
    },
    {
     "data": {
      "text/plain": "  allvecs: [array([0., 1., 2.]), array([-0.06754013,  0.48625214,  1.13305048]), array([-0.82685114, -0.1968322 ,  0.76118566]), array([-1.24393853, -0.11150447,  0.72788902]), array([-1.54894878, -0.29173794,  0.85639069]), array([-1.50040421, -0.24987256,  0.83326477]), array([-1.49999619, -0.2499999 ,  0.8333318 ]), array([-1.5       , -0.25000001,  0.83333333])]\n      fun: 4.0254032727409136e-16\n hess_inv: array([[ 1.75042228,  0.25044351, -0.25013198],\n       [ 0.25044351,  0.43758819, -0.25023187],\n       [-0.25013198, -0.25023187,  0.19446266]])\n      jac: array([-4.67069437e-09, -8.19504879e-08, -1.27256672e-07])\n  message: 'Optimization terminated successfully.'\n     nfev: 8\n      nit: 7\n     njev: 8\n   status: 0\n  success: True\n        x: array([-1.5       , -0.25000001,  0.83333333])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as ppl\n",
    "from scipy import optimize as opt\n",
    "\n",
    "# defines the distance squared function\n",
    "def distance_sq(x):\n",
    "    eq = (2-2*x[1]-3*x[2])**2 + 1/4*(1-x[0]-3*x[2])**2 + 1/9*(-2-x[0]-2*x[1])**2\n",
    "    return eq\n",
    "# defines the gradient of the distance squared function\n",
    "def distance_sq_g(x):\n",
    "    der = np.zeros_like(x)\n",
    "    der[0] = -1/2*(1-x[0]-3*x[2]) - 2/9*(-2-x[0]-2*x[1])\n",
    "    der[1] = -4*(2-2*x[1]-3*x[2]) -4/9*(-2-x[0]-2*x[1])\n",
    "    der[2] = -6*(2-2*x[1]-3*x[2]) - 3/2*(1-x[0]-3*x[2])\n",
    "    return der\n",
    "def distance_sq_H(x):\n",
    "    H = np.zeros((3,3))\n",
    "    H = H+np.diag([1/2+2/9, 8+8/9, 18+9/2])\n",
    "    H[0,1] = 4/9\n",
    "    H[1,0] = H[0,1]\n",
    "    H[0,2] = 3/2\n",
    "    H[2,0]= H[0,2]\n",
    "    H[1,2] = 12\n",
    "    H[2,1] = H[1,2]\n",
    "    return H\n",
    "x0 = [0, 1, 2]\n",
    "resgrad = opt.minimize(distance_sq,x0,method='BFGS' , jac= distance_sq_g,options={'display':True, 'return_all':True})\n",
    "xtrial = resgrad.allvecs\n",
    "xfinal = resgrad.x\n",
    "resgrad\n",
    "\n",
    "#resnewton = opt.minimize(distance_sq,x0, method='Newton-CG', jac=distance_sq_g,hess=distance_sq_H, options={'xtol':1e-8, 'disp':True})\n",
    "#resnewton.x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 3\n",
    "Prove that a hyperplane is a convex set\n",
    "A hyperplane can be defined using $ a^Tx = c \\ $ for $x \\in \\mathbb{R}^n\\ $ where a is the normal direction of the hyperplane and c is some constant.<br>\n",
    "$a^Tx = c \\ $ is a linear equation as it would multiply out as follows:\n",
    "\\begin{equation} a_1x_1 + a_2x_2 + ... +a_nx_n = c \\end{equation} this is a linear equation and linear equations contain convex sets.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}